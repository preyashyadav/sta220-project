{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping Daily Mail Online News Articles and their Comments**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4-tGRMAFvWHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjc8K7fXCJ2X",
        "outputId": "20db9d67-49fc-4dbb-be7f-02572a446c31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n",
            "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.29.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
        "\n",
        "# Base url of Daily Mail News website\n",
        "base_url = \"https://www.dailymail.co.uk\"\n",
        "# Search url for news articles\n",
        "search_url = f\"{base_url}/home/search.html\"\n",
        "\n",
        "headers = {\n",
        "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
        "    \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
        "    \"accept-language\": \"en-US,en;q=0.9\",\n",
        "    \"cache-control\": \"max-age=0, no-cache, no-store\",\n",
        "    \"cookie\": (\n",
        "        \"dm_clientsegment=c; _ga=GA1.1.920993499.1742001073; blaize_session=325103a1-fa7c-477a-8231-f1655c15a09c; \"\n",
        "        \"blaize_tracking_id=fe50a3a4-8328-487e-ad82-793f022b8192; _gcl_au=1.1.818330132.1742001073; DM_SitId845=1; \"\n",
        "        \"DM_SitId845SecId11530=1; mol.ads.visits=0; mol.ads.visitsExpire=Tue, 15 Apr 2025 01:11:14 GMT; usprivacy=1YNN; \"\n",
        "        \"uuid=75ec4cd0-8cbb-402f-8f2d-11f2ff1ac591; _li_dcdm_c=.dailymail.co.uk; _lc2_fpi=50b86ba02c46--01jpbnnw4gjcwatc875nwjrvzp; \"\n",
        "        \"_lc2_fpi_js=50b86ba02c46--01jpbnnw4gjcwatc875nwjrvzp; _pubcid=81af5535-b661-48a4-947a-071f809a73e1; \"\n",
        "        \"pbjs-unifiedid=%7B%22TDID%22%3A%22e60b74ba-f4cc-4377-9a51-df5cf9d838fa%22%2C%22TDID_LOOKUP%22%3A%22TRUE%22%2C%22TDID_CREATED_AT%22%3A%222025-02-15T01%3A11%3A13%22%7D; \"\n",
        "        \"pbjs-unifiedid_cst=3yxgLFoszg%3D%3D; _lc2_fpi_meta=%7B%22w%22%3A1742001076503%7D; _lr_env_src_ats=false; \"\n",
        "        \"__idcontext=eyJjb29raWVJRCI6IjJ1S2RDelNpdGxXZFFqWVFCVHgxSEU0RFhoSyIsImRldmljZUlEIjoiMnVLZEN3VkJidzFKZWlhaU92VmE5ejV5dHRSIiwiaXYiOiIiLCJ2IjoiIn0%3D; \"\n",
        "        \"panoramaId_expiry=1742605876095; _cc_id=b44859ccddc1206841a20a8c20ed8c9b; panoramaId=22e5322e385bfd50d3c32c04cab8185ca02cebd701bec27e0514e45437487f0b; \"\n",
        "        \"_pubcid=81af5535-b661-48a4-947a-071f809a73e1; JSESSIONID=9F3E07D52294AFF427B1E00064A2AFCC; DM_SitId845SecId4626=1; \"\n",
        "        \"permutive-id=06ec1b79-cef4-4121-a8dd-effe63f75698; cnx_userId=2-d1839001a5b64fdeb6e7fa952cd0ce70; _iiq_ab_map=%7B%2295%22%3A%22A%22%7D; \"\n",
        "        \"_au_1d=AU1D-0100-001742001141-4TO7KK6Q-KGYW; _iiq_fdata=%7B%22pcid%22%3A%229cb73a89-acbc-60e6-231a-8a992b190ce1%22%2C%22pcidDate%22%3A1742001137366%2C%22uspapi_value%22%3A%221YNN%22%2C%22sCal%22%3A1742001140836%2C%22isOptedOut%22%3Afalse%2C%22dbsaved%22%3A%22false%22%2C%22group%22%3A%22B%22%7D; \"\n",
        "        \"_au_1d=AU1D-0100-001742001141-4TO7KK6Q-KGYW; DM_SitId845SecId4637=1; _li_ss=CrYBCgYI-QEQpRoKBgj3ARClGgoFCAoQpRoKBgjdARClGgoGCPgBEKUaCgUICRClGgoGCIEBEKUaCgUIDBCvGgoGCPUBEKUaCgkI_____wcQrxoKBQgLEKUaCgYI4wEQpRoKBgikARClGgoGCLMBEKUaCgYIiQEQpRoKBgilARClGgoGCIACEKcaCgYI4QEQpRoKBgiiARClGgoGCP8BEKUaCgYI0gEQpRoKBQh-EKUaCgYIiAEQpRoSoAENlgmA6BKYAQoGCMoBEKYaCgYI5AEQphoKBgiTARCkGgoGCMkBEKYaCgYIpQEQphoKBgj0ARCkGgoGCJQBEKQaCgYIzAEQqBoKBgjGARClGgoGCMcBEKQaCgYImgEQphoKBgjnARCmGgoGCMgBEKYaCgYI5QEQphoKBgjFARCmGgoGCKwBEKYaCgYI5gEQphoKBgj-ARCmGgoGCOgBEKYaEjcNjQcI9hIwCgYIygEQphoKBgjJARCmGgoGCMUBEKYaCgYIxgEQpRoKBgjHARCkGgoGCMgBEKYaEjcNlfG4nhIwCgYIygEQphoKBgjJARCmGgoGCMUBEKYaCgYIxgEQpRoKBgjHARCkGgoGCMgBEKYa;\"\n",
        "    ),\n",
        "    \"priority\": \"u=0, i\",\n",
        "    \"referer\": \"https://www.dailymail.co.uk/home/search.html?offset=0&size=50&sel=site&searchPhrase=tesla&sort=recent&channel=news&type=article&topic=Tesla&days=last90days\",\n",
        "    \"sec-ch-ua\": '\"Not(A:Brand\";v=\"99\", \"Google Chrome\";v=\"133\", \"Chromium\";v=\"133\"',\n",
        "    \"sec-ch-ua-mobile\": \"?0\",\n",
        "    \"sec-ch-ua-platform\": '\"Windows\"',\n",
        "    \"sec-fetch-dest\": \"document\",\n",
        "    \"sec-fetch-mode\": \"navigate\",\n",
        "    \"sec-fetch-site\": \"same-origin\",\n",
        "    \"upgrade-insecure-requests\": \"1\",\n",
        "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# List to store all the fetched news links\n",
        "all_news_links = []\n",
        "# List to hold rows for CSV output\n",
        "csv_rows = []\n",
        "\n",
        "# --- Part 1: Scrape search results using Requests and Beautiful Soup ---\n",
        "with requests.Session() as session:\n",
        "    for offset in range(0, 151, 50):\n",
        "        params = {\n",
        "            \"offset\": offset,\n",
        "            \"size\": \"50\",\n",
        "            \"sel\": \"site\",\n",
        "            \"searchPhrase\": \"tesla\",\n",
        "            \"sort\": \"recent\",\n",
        "            \"channel\": \"news\",\n",
        "            \"type\": \"article\",\n",
        "            \"topic\": \"Tesla\",\n",
        "            \"days\": \"last90days\"\n",
        "        }\n",
        "        try:\n",
        "            response = session.get(search_url, headers=headers, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching search results at offset {offset}\")\n",
        "            continue\n",
        "\n",
        "        html = response.text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        for container in soup.find_all(\"div\", class_=\"sch-result\"):\n",
        "            title_section = container.find(\"h3\", class_=\"sch-res-title\")\n",
        "            if title_section:\n",
        "                a_tag = title_section.find(\"a\", href=True)\n",
        "                if a_tag:\n",
        "                    all_news_links.append(a_tag[\"href\"])\n",
        "\n",
        "# --- Part 2: Set up Selenium for article pages ---\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "try:\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    # Optional: set a page load timeout\n",
        "    driver.set_page_load_timeout(30)\n",
        "except WebDriverException as e:\n",
        "    print(f\"Error creating Selenium WebDriver\")\n",
        "    exit(1)\n",
        "\n",
        "# --- Part 3: Process each article page with Selenium ---\n",
        "with requests.Session() as session:\n",
        "    for link in all_news_links:\n",
        "        full_url = link if link.startswith(\"http\") else base_url + link\n",
        "        print(\"\\nFetching article details from:\", full_url)\n",
        "        try:\n",
        "            driver.get(full_url)\n",
        "            # Allow some extra time for dynamic content to load\n",
        "            time.sleep(2)\n",
        "            article_html = driver.page_source\n",
        "        except TimeoutException as e:\n",
        "            print(f\"Timeout loading page {full_url}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading page {full_url}\")\n",
        "            continue\n",
        "\n",
        "        article_soup = BeautifulSoup(article_html, \"html.parser\")\n",
        "\n",
        "        # Extract JSON-LD details for article metadata\n",
        "        ld_script = article_soup.find(\"script\", type=\"application/ld+json\")\n",
        "        if ld_script:\n",
        "            try:\n",
        "                ld_data = json.loads(ld_script.string.strip())\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing JSON-LD from {full_url}\")\n",
        "                continue\n",
        "            date_published = ld_data.get(\"datePublished\", \"N/A\")\n",
        "            author = ld_data.get(\"author\")\n",
        "            if isinstance(author, dict):\n",
        "                author_name = author.get(\"name\", \"N/A\")\n",
        "            elif isinstance(author, list):\n",
        "                author_name = \", \".join(item.get(\"name\", \"N/A\") for item in author)\n",
        "            else:\n",
        "                author_name = \"N/A\"\n",
        "        else:\n",
        "            date_published = \"N/A\"\n",
        "            author_name = \"N/A\"\n",
        "\n",
        "        # Extract article content\n",
        "        article_body_div = article_soup.find(\"div\", itemprop=\"articleBody\")\n",
        "        paragraphs = []\n",
        "        if article_body_div:\n",
        "            for p in article_body_div.find_all(\"p\"):\n",
        "                paragraphs.append(p.get_text(strip=True))\n",
        "        article_text = \" \".join(paragraphs)\n",
        "\n",
        "        csv_rows.append({\n",
        "            \"type\": \"news\",\n",
        "            \"content\": article_text,\n",
        "            \"createdAt\": date_published,\n",
        "            \"Author Name\": author_name,\n",
        "            \"url\": full_url\n",
        "        })\n",
        "\n",
        "        # --- Fetch comments ---\n",
        "        match = re.search(r'article-(\\d+)', full_url)\n",
        "        if match:\n",
        "            article_id = match.group(1)\n",
        "            comments_url = f\"{base_url}/reader-comments/p/asset/readcomments/{article_id}?max=999&offset=0&order=desc\"\n",
        "            comments_headers = {\n",
        "                \"User-Agent\": headers[\"user-agent\"],\n",
        "                \"Accept\": \"application/json, text/plain, */*\",\n",
        "                \"Referer\": full_url,\n",
        "                \"sec-ch-ua\": headers[\"sec-ch-ua\"],\n",
        "                \"sec-ch-ua-mobile\": headers[\"sec-ch-ua-mobile\"],\n",
        "                \"sec-ch-ua-platform\": headers[\"sec-ch-ua-platform\"]\n",
        "            }\n",
        "            try:\n",
        "                comments_response = session.get(comments_url, headers=comments_headers, timeout=10)\n",
        "                comments_response.raise_for_status()\n",
        "                comments_data = comments_response.json()\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching comments from {comments_url}\")\n",
        "                continue\n",
        "\n",
        "            # Process and save comments\n",
        "            def process_comment(comment, indent=0):\n",
        "                user_alias = comment.get(\"userAlias\", \"\")\n",
        "                message = comment.get(\"message\", \"\")\n",
        "                date_created = comment.get(\"dateCreated\", \"\")\n",
        "                csv_rows.append({\n",
        "                    \"type\": \"comment\",\n",
        "                    \"content\": message,\n",
        "                    \"createdAt\": date_created,\n",
        "                    \"Author Name\": user_alias,\n",
        "                    \"url\": full_url\n",
        "                })\n",
        "                replies = comment.get(\"replies\", {}).get(\"comments\", [])\n",
        "                for reply in replies:\n",
        "                    process_comment(reply, indent=indent+4)\n",
        "\n",
        "            for comment in comments_data.get(\"payload\", {}).get(\"page\", []):\n",
        "                process_comment(comment)\n",
        "        else:\n",
        "            print(\"No article ID found in URL for comments fetching.\")\n",
        "\n",
        "# --- Cleanup Selenium ---\n",
        "try:\n",
        "    driver.quit()\n",
        "except Exception as e:\n",
        "    print(f\"Error quitting WebDriver\")\n",
        "\n",
        "# --- Part 4: Save CSV ---\n",
        "csv_filename = \"output.csv\"\n",
        "try:\n",
        "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"type\", \"content\", \"createdAt\", \"Author Name\", \"url\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in csv_rows:\n",
        "            writer.writerow(row)\n",
        "    print(f\"\\nData has been saved to {csv_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing CSV file\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOd23Lnw9UxK",
        "outputId": "ff5caf23-a5e0-4ef1-ed4c-cb8c03ef2953"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14479011/Jennifer-Saunders-Adrian-Edmondsons-Absolutely-Fabulous-plans-2million-Dartmoor-mansion-garden-pavilion-pottery-workshop.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14501799/elon-musk-mars-spacex-starship-optimus-2026.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14501189/Met-Police-thousands-protesters-descend-London-weekend-demos-pro-Palestine-anti-Musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14500525/mark-kelly-senator-selling-tesla-elon-musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14497841/James-Murdoch-Tesla.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14498041/Thief-smashed-Tesla-stole-1-350-Stone-Island-jacket-Id-bought-Harrods-took-try-refund.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14497375/elon-musk-tesla-letter-donald-trump-tariffs-stock-price.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14493705/Tesla-records-cyclist-smashes-window-steal-1000-Stone-Island-jacket-Harrods.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14493705/Tesla-records-cyclist-smashes-window-steal-1000-Stone-Island-jacket-Harrods.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14492077/Trumps-brand-new-red-Tesla-recalled-revealed-wholl-actually-driving-car.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14490375/Elon-Musks-Optimus-robot-targeted-Just-Stop-Oil.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14483677/Elon-Musks-DOGE-approval-rating-revealed-voters-predict-true-intentions-Trump.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14488877/Elon-Musk-trans-daughter-Vivian-Wilson-IVF-shock-claim.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14488561/Trump-recession-stock-market-trade-war-mexico-canada.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14488025/donald-trump-tesla-domestic-terrorism-elon-musk.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14488025/donald-trump-tesla-domestic-terrorism-elon-musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14487443/Trump-tests-brand-new-Teslas-White-House-lawn-backs-buddy-Elon-Musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14487043/lara-trump-gushes-elon-musk-reveals-kissing-feet.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14487043/lara-trump-gushes-elon-musk-reveals-kissing-feet.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/debate/article-14485697/Elon-Musk-liberals-Telsa-Cybertrucks.html\n",
            "Timeout loading page https://www.dailymail.co.uk/debate/article-14485697/Elon-Musk-liberals-Telsa-Cybertrucks.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14483177/Elon-Musk-tears-George-Soros-accuses-Reid-Hoffman-Epsteins-list-anti-Tesla-plot.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14484697/Donald-Trump-buy-brand-new-Tesla-support-Elon-Musk-baby.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14484697/Donald-Trump-buy-brand-new-Tesla-support-Elon-Musk-baby.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14481765/Anti-Musk-protestors-attack-Tesla-showrooms-Molotov-cocktails-guns.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14478697/snl-skit-elon-musk-dr-evil.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14471951/Trump-picks-sides-MAGA-power-struggle-Elon-Musk-Cabinet.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14471951/Trump-picks-sides-MAGA-power-struggle-Elon-Musk-Cabinet.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14466507/The-brutal-truth-Anthony-Albaneses-failing-masterplan-drive-Australians-Teslas-EVs.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14466507/The-brutal-truth-Anthony-Albaneses-failing-masterplan-drive-Australians-Teslas-EVs.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14469485/elon-musk-daylight-savings-poll-furious-debate-america.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14468785/elon-musk-gesture-cancer-dj-daniel-target-msnbc-nicole-wallace.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14462407/Steve-Wozniak-Apple-big-tech-Elon-Musk.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14462407/Steve-Wozniak-Apple-big-tech-Elon-Musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14459827/Piedmont-California-cybertruck-crash-victim-rescued.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14459107/Tesla-showroom-torched-arson-attack-France-550-000-cars-destroyed-amid-growing-fury-Europe-Elon-Musks-EU-meddling-Trump-Ukraine-funding-row.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14458825/Elon-Musk-fellow-Royal-Society-crisis-meeting-expulsion.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14456035/cause-tesla-cybertruck-crash-teens-die-Piedmont-california.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14454873/protests-against-Elon-Musk-JD-Vance-spread-America.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14437853/Tesla-drivers-vow-sell-cars-Elon-Musk-support-far-rights-slump.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14437853/Tesla-drivers-vow-sell-cars-Elon-Musk-support-far-rights-slump.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14450431/elon-musk-ponzi-scheme-joe-rogan-podcast-doge.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14450423/Climate-protesters-occupy-Tesla-showroom-Westfield-Elon-Musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14449609/Dave-Portnoy-Trump-administration-Elon-musk-Tesla.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14449609/Dave-Portnoy-Trump-administration-Elon-musk-Tesla.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14449499/elon-musk-court-child-ashley-st-clair.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14442683/man-smashes-electric-car-protest-Elon-Musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14440059/epa-slash-federal-workers-climate-trump-musk.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14440059/epa-slash-federal-workers-climate-trump-musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14435635/elon-musk-war-ashley-st-clair-custody-new-york.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14435635/elon-musk-war-ashley-st-clair-custody-new-york.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14436805/influencer-filmed-calling-Elon-Musk-assassination-admitting-not-paying-tax-tiktok-ed-martin.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14436805/influencer-filmed-calling-Elon-Musk-assassination-admitting-not-paying-tax-tiktok-ed-martin.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14435763/doge-protests-town-hall-elon-musk-congress.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14434627/Tesla-sales-slump-Europe-Elon-Musk-political-meddling.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14429783/elon-musk-warning-federal-workers-office.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14428293/elon-musk-real-reason-five-tasks-pentagon-criticism.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14426887/elon-musk-doge-federal-workers-recap-email.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14426887/elon-musk-doge-federal-workers-recap-email.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14424391/elon-musk-messages-ashley-st-clair-revealed.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14424273/elon-musk-baby-mama-ashley-st-clair-lawsuit-custody-son.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14424273/elon-musk-baby-mama-ashley-st-clair-lawsuit-custody-son.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14419343/grimes-message-elon-musk-child-suffers-medical-crisis.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14418583/Pete-Hegseth-orders-DOGE-inspired-50-billion-Pentagon-cuts.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14418583/Pete-Hegseth-orders-DOGE-inspired-50-billion-Pentagon-cuts.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14416059/elon-musk-warns-america-collapse-doge.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14412993/Elon-Musk-reason-endorsed-Donald-Trump-president.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14412993/Elon-Musk-reason-endorsed-Donald-Trump-president.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14411673/Donald-Trump-Elon-Musk-job-federal-government.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14411673/Donald-Trump-Elon-Musk-job-federal-government.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14408629/elon-musk-ashley-st-clair-ghosted.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14406287/archbishop-bold-message-elon-musk-13-children-4-women.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14405589/elon-musk-rant-tucker-carlson-democrats-ashley-st-clair.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14403685/nick-cannon-father-elon-musk-child.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14403031/elon-musk-ashley-st-clair-tirade-milo-yiannopoulos.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14402765/elon-musk-breaks-silence-maga-influencer-secret-baby.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14393751/Elon-Musk-father-Errol-shocking-claims-child-death.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14394769/elon-musk-kids-work-son-x-viral-oval-office-performance.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14391683/Elon-Musk-matrix-DOGE-mass-layoffs.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14391683/Elon-Musk-matrix-DOGE-mass-layoffs.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14389609/Democrat-protest-song-Elon-Musk-mocked-Trump.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14389487/elon-musk-spacex-awarded-millions-contract-donald-trump.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14386173/Sydney-neighbour-keyed-Tesla.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14386173/Sydney-neighbour-keyed-Tesla.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14383145/Tesla-sales-slump-Australia.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14383145/Tesla-sales-slump-Australia.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14377825/trump-reveals-elon-musk-doge-target-department-education-spending-cuts.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14377739/Elon-Musks-DOGE-Treasury-judge-blocks-access.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14377739/Elon-Musks-DOGE-Treasury-judge-blocks-access.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14361093/boston-university-threat-elon-musk-jared-may.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14372129/Driver-Tesla-Cybertruck-seized-police-Manchester.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14372129/Driver-Tesla-Cybertruck-seized-police-Manchester.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14368445/elon-musk-doge-lieutenant-accused-wife-baby-government-building.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14368445/elon-musk-doge-lieutenant-accused-wife-baby-government-building.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14369147/doge-artificial-intelligence-federal-government-elon-musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14367265/Elon-Musk-wades-Sweden-shooting-debate-reposting-claim-legacy-media-silent-massacre-headlines-world.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14367265/Elon-Musk-wades-Sweden-shooting-debate-reposting-claim-legacy-media-silent-massacre-headlines-world.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14363699/Elon-Musk-doge-nerd-Gavin-Kliger-substack-troll-usaid.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14363699/Elon-Musk-doge-nerd-Gavin-Kliger-substack-troll-usaid.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14361333/AOC-ridiculed-bizarre-Elon-Musks-intelligence-billionaire-Buddy-guts-government.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14361333/AOC-ridiculed-bizarre-Elon-Musks-intelligence-billionaire-Buddy-guts-government.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14358977/Woman-killed-fire-ball-crashing-Tesla-guardrail-busy-NYC-road.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14357585/Elon-musk-doge-college-aged-employees.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14340385/donald-trump-offers-elon-musk-lincoln-bedroom-white-house.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14337749/Elon-Musk-says-sue-Tim-Walz-said-Tesla-CEO.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14336223/Elon-Musk-role-reshaping-federal-government-purge.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14325817/software-engineer-Mikhael-Misha-Romanenko-killed-huge-Tesla-crash.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14322131/Netanyahu-defends-Elon-Musk-Nazi-salute-falsely-smeared.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14322131/Netanyahu-defends-Elon-Musk-Nazi-salute-falsely-smeared.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14319789/trump-losing-control-elon-musk.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14319789/trump-losing-control-elon-musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14318435/boyfriend-final-words-killed-tesla-linh-luu.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14318435/boyfriend-final-words-killed-tesla-linh-luu.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14317317/Elon-Musk-controversial-salute-projected-German-car-factory.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14315167/elon-musk-transgender-daughter-vivian-fiery-response-controversial-inauguration-salute.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14308055/elon-musk-defends-hand-gesture-adl-nazi-salute.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14306167/elon-musk-barron-trump-inauguration-viral-video.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14303257/Elon-Musks-adorable-toddler-X-steals-thunder-Trumps-pre-inauguration-rally.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14293985/elon-musk-doge-bust-feds-green-scheme-billions-taxpayer-money.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14296291/cops-seize-banned-Tesla-cybertruck-road-legal-UK.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14296291/cops-seize-banned-Tesla-cybertruck-road-legal-UK.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14286545/Phones4U-billionaire-John-Caudwell-Tesla-Model-X-day-warranty-ran-out.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14268251/Elon-Musks-tweets-probed-UK-counter-extremism-unit-grooming-gangs.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14259165/Rotherham-grooming-gang-survivor-rapist-mile-house-released-prison-blasts-Keir-Starmer-point-scoring-insulting-public-scandal.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14266971/elon-musk-doge-plan-save-taxpayers-trillion.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14263483/Elon-Musk-hatred-attacking-institutions-Spanish-PM-Sanchez-European-backlash-tesla.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14261377/las-vegas-cybertruck-bomber-ai-trump-hotel-explosion.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14261377/las-vegas-cybertruck-bomber-ai-trump-hotel-explosion.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14260697/elon-musk-biographer-astonishing-claim-health.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14258985/Elon-Musk-English-nana-grooming-gang-victim.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14258985/Elon-Musk-English-nana-grooming-gang-victim.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14258705/Cybertruck-bomber-matthew-livelsberger-candy-man-afghan-children-deployment.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14258177/melania-trump-donald-bromance-elon-musk-thinks.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/tvshowbiz/article-14256901/EDEN-CONFIDENTIAL-Peacemaker-Duke-Marlborough-tries-unite-Elon-Musk-Nigel-Farage-inviting-billionaire-Tesla-tycoon-Blenheim-Palace.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14256619/Keir-Starmer-elon-musk-smear-tactics-grooming-gang-inquiry-Rochdale.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14256619/Keir-Starmer-elon-musk-smear-tactics-grooming-gang-inquiry-Rochdale.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14254539/Police-force-quits-Elon-Musks-X-quantity-quality-responses-receives-latest-organisation-leave-billionaires-site.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14253621/Elon-Musk-locks-reporter-X-article-sheds-light-Tesla-owners-alleged-alter-ego-Adrian-Dittman.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14249771/terrifying-moment-self-driving-tesla-train-tracks-ceo.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14247511/Elon-Musk-powerful-unelected-man-world-Starmer-Trump-Buddy-save-planet.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14247341/Elon-Musk-grooming-gangs-Keir-Starmer.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14247089/Matthew-Livelsberger-map-America-charge-Cybertruck-Vegas-explosion.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14247109/uncle-las-vegas-cybertruck-bomb-matthew-livelsberger-statement-explosion.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14246623/Elon-Musk-calls-King-Charles-order-new-general-election-pressure-mounts-Keir-Starmer-grooming-gangs-inquiry.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14246183/Cybertruck-bomber-texts-ex-girlfriend.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14246001/Las-Vegas-Cybertruck-bomber-Matthew-Livelsburger-left-wife-argument-infidelity.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14245695/las-vegas-cybertruck-bomber-matt-livelsberger-wife-trump-tower-explosion-las-vegas-cybertruck-bomber-matt-livelsberger-wife-trump-tower-explosion.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14245695/las-vegas-cybertruck-bomber-matt-livelsberger-wife-trump-tower-explosion-las-vegas-cybertruck-bomber-matt-livelsberger-wife-trump-tower-explosion.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14245379/Green-Beret-Matthew-Livelsberger-military-record.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14244969/Cybertruck-bomber-Matthew-Livelsbergers-stunned-family-say-loved-Trump-army.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14244903/Matthew-Livelsberger-vegas-trump-hotel-new-orleans.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14244903/Matthew-Livelsberger-vegas-trump-hotel-new-orleans.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14244257/Cybertruck-bomber-Matthew-Livelsbergers-wife-warning-sign-social-media-posts.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14244257/Cybertruck-bomber-Matthew-Livelsbergers-wife-warning-sign-social-media-posts.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14243689/matthew-livelsberger-las-vegas-tesla-cybertruck-explosion-colorado-springs.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14243381/Donald-Trump-breaks-silence-Tesla-Cybertruck-explosion-outside-Las-Vegas-hotel-vows-rid-America-violent-scum.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14243295/Pictured-Las-Vegas-bomber-blew-Cybertruck.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14243295/Pictured-Las-Vegas-bomber-blew-Cybertruck.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14243099/cybertruck-explosion-suspect-identified-las-vegas.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14242845/las-vegas-barricaded-tesla-cybertruck.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14242845/las-vegas-barricaded-tesla-cybertruck.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14242509/melania-breaks-silence-cybertruck-trump-hotel-las-vegas.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14242697/cybertruck-explosives-las-vegas-terror-links.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14242697/cybertruck-explosives-las-vegas-terror-links.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14242603/joe-biden-addresses-new-orleans-terror-attack-las-vegas-cybertruck.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14239465/Chancellor-Scholz-fires-Elon-Musk-begs-voters-not-let-owners-social-media-channels-decide-elections-Tesla-boss-insisted-AfD-save-Germany.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14239465/Chancellor-Scholz-fires-Elon-Musk-begs-voters-not-let-owners-social-media-channels-decide-elections-Tesla-boss-insisted-AfD-save-Germany.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14237951/Elon-Musk-renting-Mar-Lago-cottage-Donald-Trump.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14237571/Tesla-driver-Luke-Erwin-Gold-Coast-stranger-kick-car.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14236395/Labour-MPs-left-wing-Starmer-Elon-Musk-companies-Britain-Badenoch.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14235143/Elon-Musk-launches-savage-attack-Sydney-Morning-Herald.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14233385/EV-idle-fees-Chadstone.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14233385/EV-idle-fees-Chadstone.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14232687/trump-breaks-silence-h1b-visas-elon-musk.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14232687/trump-breaks-silence-h1b-visas-elon-musk.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14232525/sarah-palin-joins-maga-h1b-elon-musk-donald-trump.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14227309/Elon-Musk-Jeff-Bezos-ex-wife-MacKenzie-brutal-quip.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14225421/Elon-Musk-Optimus-robots-Christmas-card-Starbase-Texas.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14222015/don-lemon-cnn-questions-backfire-live-interview-john.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14220185/Tesla-keyed-old-man-Rose-Bay-Hotel.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14220185/Tesla-keyed-old-man-Rose-Bay-Hotel.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14219739/voter-brands-yvette-cooper-two-faced-pies-Christmas-blanket.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14219739/voter-brands-yvette-cooper-two-faced-pies-Christmas-blanket.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14217061/tesla-recalls-vehicles-tire-pressure-software-update-issue.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14214737/Elon-Musk-tweets-AfD-save-Germany-sparking-fury-countrys-political-groups-linked-Nigel-Farage.html\n",
            "\n",
            "Fetching article details from: https://www.dailymail.co.uk/news/article-14205963/california-cybertruck-owner-loloct7-license-plate.html\n",
            "Timeout loading page https://www.dailymail.co.uk/news/article-14205963/california-cybertruck-owner-loloct7-license-plate.html\n",
            "\n",
            "Data has been saved to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis of Scraped Articles**"
      ],
      "metadata": {
        "id": "BzkgY1oMP-2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwLhOac30Vbv",
        "outputId": "2a580642-8f2a-4f79-f213-ee3caecb7898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vader\n",
            "  Downloading vader-0.0.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from vader) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vader) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vader) (1.14.1)\n",
            "Collecting sonopy (from vader)\n",
            "  Downloading sonopy-0.1.2.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->vader) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->vader) (3.5.0)\n",
            "Downloading vader-0.0.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sonopy\n",
            "  Building wheel for sonopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sonopy: filename=sonopy-0.1.2-py3-none-any.whl size=2851 sha256=ac044f4593dba074f9b09723df9453c801ad55a00c7a8da9c5152068e048cece\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/02/0d/df138747348c15908c1fb09493064ead497e16e187e3094d71\n",
            "Successfully built sonopy\n",
            "Installing collected packages: sonopy, vader\n",
            "Successfully installed sonopy-0.1.2 vader-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import shutil\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# To download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Input folder path containing the CSV or XLSX files\n",
        "input_folder = '/content/Articles_content'\n",
        "\n",
        "# Output folder for saving processed output CSV files\n",
        "output_folder = '/content/Articles_content_with_sentiment'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Text data preprocessing (stripping extra whitespace)\n",
        "def data_preprocessing(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.strip()\n",
        "    return text\n",
        "\n",
        "# Get all the CSV and XLSX files in the input folder\n",
        "csv_files = glob.glob(os.path.join(input_folder, \"*.csv\"))\n",
        "xlsx_files = glob.glob(os.path.join(input_folder, \"*.xlsx\"))\n",
        "files = csv_files + xlsx_files\n",
        "\n",
        "# Iterate over each file for performing sentiment analysis\n",
        "for file in files:\n",
        "    filename = os.path.basename(file)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    if ext.lower() == \".csv\":\n",
        "        df = pd.read_csv(file)\n",
        "    elif ext.lower() in [\".xlsx\", \".xls\"]:\n",
        "        df = pd.read_excel(file)\n",
        "    else:\n",
        "        continue  # Skip files of other formats\n",
        "\n",
        "    # Calculate the compound sentiment score for each row's \"content\"\n",
        "    df['sentiment'] = df['content'].apply(\n",
        "        lambda text: sid.polarity_scores(data_preprocessing(text))['compound']\n",
        "        if isinstance(text, str) else None\n",
        "    )\n",
        "\n",
        "    print(f\"Sentiment analysis for {filename}:\")\n",
        "    print(df[['content', 'sentiment']].head())\n",
        "\n",
        "    # Save the updated DataFrame to a CSV file in output folder\n",
        "    output_file = os.path.join(output_folder, f\"{name}_with_sentiment.csv\")\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Output saved to {output_file}\")\n",
        "\n",
        "# Compress to zip and download output folder (running on local)\n",
        "zip_filename = os.path.basename(os.path.normpath(output_folder)) + '.zip'\n",
        "shutil.make_archive(os.path.splitext(zip_filename)[0], 'zip', output_folder)\n",
        "print(f\"\\nOutput folder compressed to {zip_filename}\")\n",
        "\n",
        "# If running in google colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab. Please manually download the ZIP file from your file system.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "TxLb5ncOvFL8",
        "outputId": "dba1975e-127f-4b31-e618-a5b7811dcb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis for DailyMail_Articles.csv:\n",
            "                                             content  sentiment\n",
            "0  Salty people with TDS don't know new cars from...     0.0000\n",
            "1  I’m sure you are happy to pay 100k for a car w...     0.8910\n",
            "2  He certainly didn’t instill confidence for the...     0.6908\n",
            "3  The most successful businessman on the planet,...    -0.6915\n",
            "4  Plus SpaceX was canceled today 30 minutes befo...     0.0000\n",
            "Output saved to /content/Articles_content_with_sentiment/DailyMail_Articles_with_sentiment.csv\n",
            "Sentiment analysis for reddit_tslamotors_elonmusk_Jan1_Mar12.xlsx:\n",
            "                                             content  sentiment\n",
            "0  Yes, not in the U.S.. It used to be a Toyota C...     0.4019\n",
            "1  I think this is many people's want, would make...     0.5719\n",
            "2        Is it limited by the pack or the inverters?    -0.2263\n",
            "3  I'm 6'4\" and 225lbs, drove both, and prefer th...     0.8658\n",
            "4  He literally described every hard working emig...    -0.2023\n",
            "Output saved to /content/Articles_content_with_sentiment/reddit_tslamotors_elonmusk_Jan1_Mar12_with_sentiment.csv\n",
            "Sentiment analysis for reddit_tsla_Jan1_Mar12.xlsx:\n",
            "                                             content  sentiment\n",
            "0  The delivery number has been revised downward ...     0.6705\n",
            "1  \"I'll buy when it hits $100\"\\n\\nVibes 6 months...     0.0000\n",
            "2  If they lose their EV credit AND lower the pri...    -0.1779\n",
            "3                       Youâ€™re delusional. No clue    -0.2960\n",
            "4                                 Why not just hold?     0.0000\n",
            "Output saved to /content/Articles_content_with_sentiment/reddit_tsla_Jan1_Mar12_with_sentiment.csv\n",
            "\n",
            "Output folder compressed to Articles_content_with_sentiment.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_927d9b9c-07dd-4691-b8bd-7c027acda35e\", \"Articles_content_with_sentiment.zip\", 6750643)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YouTube**"
      ],
      "metadata": {
        "id": "EOj8rp49UsgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LRU1RFfXU0M",
        "outputId": "f1cb697e-65ce-4e09-b5ec-cab859c0a815"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-1.0.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (2025.1.31)\n",
            "Downloading youtube_transcript_api-1.0.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping News URLs via Youtube API**"
      ],
      "metadata": {
        "id": "4rI8v7L60f2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# YouTube Data API key\n",
        "api_key = \"AIzaSyCzduWRq-77kXXbsUt8WCUOZPONfo1UPC0\"\n",
        "\n",
        "# YouTube API client to interact with data\n",
        "youtube = build('youtube', 'v3', developerKey=api_key, cache_discovery=False)\n",
        "\n",
        "# Start and End Date range\n",
        "start_date = datetime.date(2025, 1, 1)\n",
        "end_date = datetime.date(2025, 3, 12)\n",
        "delta = datetime.timedelta(days=1)\n",
        "\n",
        "# List to store rows of data\n",
        "rows = []\n",
        "\n",
        "current_date = start_date\n",
        "while current_date <= end_date:\n",
        "    # Define the time window for the current day\n",
        "    published_after = current_date.isoformat() + \"T00:00:00Z\"\n",
        "    next_date = current_date + delta\n",
        "    published_before = next_date.isoformat() + \"T00:00:00Z\"\n",
        "\n",
        "    try:\n",
        "        # Perform a search query for \"Tesla news\" to obtain videos with captions enabled (to fetch transcripts)\n",
        "        request = youtube.search().list(\n",
        "            q=\"Tesla news\",\n",
        "            part=\"snippet\",\n",
        "            type=\"video\",\n",
        "            videoCaption=\"closedCaption\",\n",
        "            maxResults=1,\n",
        "            order=\"date\",\n",
        "            publishedAfter=published_after,\n",
        "            publishedBefore=published_before\n",
        "        )\n",
        "        response = request.execute()\n",
        "        # To avoid rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "        if response.get('items'):\n",
        "            video_id = response['items'][0]['id']['videoId']\n",
        "            video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "            print(f\"{current_date}: {video_url}\")\n",
        "        else:\n",
        "            video_url = \"\"\n",
        "            print(f\"{current_date}: No video found\")\n",
        "    except HttpError as e:\n",
        "        print(f\"HttpError on {current_date}: {e}\")\n",
        "        video_url = \"\"\n",
        "        # Longer wait time if an error occurs\n",
        "        time.sleep(10)\n",
        "\n",
        "    # Append the result for the current day\n",
        "    rows.append({\"date\": current_date.isoformat(), \"url\": video_url})\n",
        "    current_date = next_date\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"youtubeVideo_links.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"\\nCSV file saved as {output_csv}\")\n",
        "\n",
        "# To download the CSV file from Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_csv)\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab. Please download the file manually.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0HWDAskOiKAG",
        "outputId": "a19c1334-3c20-4dfe-e321-7bfb4ddf0e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-01: https://www.youtube.com/watch?v=Q4HnFkmRDCw\n",
            "2025-01-02: https://www.youtube.com/watch?v=mroFnhe6olo\n",
            "2025-01-03: https://www.youtube.com/watch?v=bQyx0F2bT9A\n",
            "2025-01-04: https://www.youtube.com/watch?v=JqxtqfjvBMA\n",
            "2025-01-05: https://www.youtube.com/watch?v=y1jrt5jKApI\n",
            "2025-01-06: https://www.youtube.com/watch?v=3Brp8xn1ko4\n",
            "2025-01-07: https://www.youtube.com/watch?v=ukkP8GtN6pk\n",
            "2025-01-08: https://www.youtube.com/watch?v=SBVNzZjoI50\n",
            "2025-01-09: https://www.youtube.com/watch?v=Q4Gh2n2HBmc\n",
            "2025-01-10: https://www.youtube.com/watch?v=xjv5uNwmHbI\n",
            "2025-01-11: https://www.youtube.com/watch?v=GgdKgb5Le1Y\n",
            "2025-01-12: https://www.youtube.com/watch?v=E9K7v4V2ZSc\n",
            "2025-01-13: https://www.youtube.com/watch?v=-n0qGeBwpTo\n",
            "2025-01-14: https://www.youtube.com/watch?v=nDTvv65b8Ys\n",
            "2025-01-15: https://www.youtube.com/watch?v=TJ2T_ITlexo\n",
            "2025-01-16: https://www.youtube.com/watch?v=ekGzJgNQrTM\n",
            "2025-01-17: https://www.youtube.com/watch?v=45FccKLoNzc\n",
            "2025-01-18: https://www.youtube.com/watch?v=cxNE0hqAfcs\n",
            "2025-01-19: https://www.youtube.com/watch?v=OF-QagxsCcU\n",
            "2025-01-20: https://www.youtube.com/watch?v=3dRVBKm2BH0\n",
            "2025-01-21: https://www.youtube.com/watch?v=5rnzskmZVB8\n",
            "2025-01-22: https://www.youtube.com/watch?v=yTz2sIv_v68\n",
            "2025-01-23: https://www.youtube.com/watch?v=T9UvsMEQotM\n",
            "2025-01-24: https://www.youtube.com/watch?v=1iG8ir-ztdo\n",
            "2025-01-25: https://www.youtube.com/watch?v=bvYQTf28zXQ\n",
            "2025-01-26: No video found\n",
            "2025-01-27: https://www.youtube.com/watch?v=9mWzibZwocg\n",
            "2025-01-28: https://www.youtube.com/watch?v=uouhMdlE2rg\n",
            "2025-01-29: https://www.youtube.com/watch?v=MAXyygpUor0\n",
            "2025-01-30: https://www.youtube.com/watch?v=_ztSlF_J6rA\n",
            "2025-01-31: https://www.youtube.com/watch?v=sOzepEHLR80\n",
            "2025-02-01: https://www.youtube.com/watch?v=dN9ZGNfkZSg\n",
            "2025-02-02: https://www.youtube.com/watch?v=0xLewVeDpUQ\n",
            "2025-02-03: https://www.youtube.com/watch?v=pQBKoWfclEM\n",
            "2025-02-04: https://www.youtube.com/watch?v=UDM_x6Snq-c\n",
            "2025-02-05: https://www.youtube.com/watch?v=wvWKb52cZ28\n",
            "2025-02-06: https://www.youtube.com/watch?v=LIP0-oLhAQQ\n",
            "2025-02-07: https://www.youtube.com/watch?v=kh1p4GDDXbI\n",
            "2025-02-08: https://www.youtube.com/watch?v=jJf18gsxFFk\n",
            "2025-02-09: https://www.youtube.com/watch?v=oEWYw3JUfME\n",
            "2025-02-10: https://www.youtube.com/watch?v=SpgQ-GWItiE\n",
            "2025-02-11: https://www.youtube.com/watch?v=WWNJtjqgURU\n",
            "2025-02-12: https://www.youtube.com/watch?v=9UWAlOvFEbI\n",
            "2025-02-13: https://www.youtube.com/watch?v=Yv9gzhoT59E\n",
            "2025-02-14: https://www.youtube.com/watch?v=2QKwZyK56nA\n",
            "2025-02-15: https://www.youtube.com/watch?v=0immYy4mVzQ\n",
            "2025-02-16: https://www.youtube.com/watch?v=SRzKCm0hmjU\n",
            "2025-02-17: https://www.youtube.com/watch?v=vSyRtyk6Cxo\n",
            "2025-02-18: https://www.youtube.com/watch?v=iMlPHsa5XdU\n",
            "2025-02-19: https://www.youtube.com/watch?v=dkdUfSzdCDM\n",
            "2025-02-20: https://www.youtube.com/watch?v=nGRSgXObUc8\n",
            "2025-02-21: https://www.youtube.com/watch?v=8vI-5zzNkP0\n",
            "2025-02-22: https://www.youtube.com/watch?v=w21TLQOIThk\n",
            "2025-02-23: https://www.youtube.com/watch?v=QA-laFwTCoc\n",
            "2025-02-24: https://www.youtube.com/watch?v=z40rbOLUi-w\n",
            "2025-02-25: https://www.youtube.com/watch?v=rLgWLXzzM0Q\n",
            "2025-02-26: https://www.youtube.com/watch?v=44UsjFNQt4Q\n",
            "2025-02-27: https://www.youtube.com/watch?v=W_aUPoFkULs\n",
            "2025-02-28: https://www.youtube.com/watch?v=gut_Gys7Pbo\n",
            "2025-03-01: https://www.youtube.com/watch?v=Kx2DG4YtbF4\n",
            "2025-03-02: https://www.youtube.com/watch?v=iHBf8Vb8bmY\n",
            "2025-03-03: https://www.youtube.com/watch?v=Pus_mbDP4Zo\n",
            "2025-03-04: https://www.youtube.com/watch?v=cKHi4cFXAc0\n",
            "2025-03-05: https://www.youtube.com/watch?v=CWze2xQaqcw\n",
            "2025-03-06: https://www.youtube.com/watch?v=Y9xwZRV12Nw\n",
            "2025-03-07: https://www.youtube.com/watch?v=Ccpr1TOzzI0\n",
            "2025-03-08: https://www.youtube.com/watch?v=UfyPtvZGHAI\n",
            "2025-03-09: https://www.youtube.com/watch?v=BI84E354Yc8\n",
            "2025-03-10: https://www.youtube.com/watch?v=AwoBakHU2vQ\n",
            "2025-03-11: https://www.youtube.com/watch?v=QYU0n9F7pHk\n",
            "2025-03-12: https://www.youtube.com/watch?v=_fgZ-idXUeY\n",
            "\n",
            "CSV file saved as youtubeVideo_links.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f7d6575e-15d5-414b-b4cb-d48267352060\", \"youtubeVideo_links.csv\", 3871)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping Transcript and Comments of Youtube News Videos on Tesla**"
      ],
      "metadata": {
        "id": "h_1ugtWfQYCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gPsSrIeuSeZz",
        "outputId": "e4ef3a60-8f03-428d-dd65-c58847ae26e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=6374b77ec8348d440ab082a575257366c5e7ed77fe3a41504e4e68147a7c2965\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.13 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "83328412363c42eab090478a0b57dbee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from googletrans import Translator  # Added for translation\n",
        "\n",
        "# Extract video ID from various YouTube URL formats\n",
        "def extract_video_id(url):\n",
        "    regex = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
        "    match = re.search(regex, url)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid YouTube URL format.\")\n",
        "\n",
        "# Retrieve the transcript for a video with translation fallback\n",
        "def get_transcript(video_id):\n",
        "    # Try to fetch the English transcript\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "        transcript_text = \" \".join([entry['text'] for entry in transcript_list])\n",
        "        return transcript_text\n",
        "    except Exception as e:\n",
        "        print(f\"English transcript not available for video {video_id}: {e}\")\n",
        "\n",
        "    # Try Spanish transcript and translate to English\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['es-419'])\n",
        "        transcript_text = \" \".join([entry['text'] for entry in transcript_list])\n",
        "        print(\"Spanish transcript retrieved. Translating to English...\")\n",
        "        translator = Translator()\n",
        "        translated = translator.translate(transcript_text, src='es', dest='en')\n",
        "        return translated.text\n",
        "    except Exception as e2:\n",
        "        print(f\"Spanish transcript not available for video {video_id}: {e2}\")\n",
        "        return None\n",
        "\n",
        "# Retrieve video comments with individual author and timestamp details\n",
        "def get_video_comments(video_id, api_key):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key, cache_discovery=False)\n",
        "    comments = []  # List to store comment details\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            request = youtube.commentThreads().list(\n",
        "                part='snippet',\n",
        "                videoId=video_id,\n",
        "                textFormat='plainText',\n",
        "                maxResults=100,\n",
        "                pageToken=next_page_token\n",
        "            )\n",
        "            response = request.execute()\n",
        "            time.sleep(1)\n",
        "        except HttpError as e:\n",
        "            error_msg = str(e)\n",
        "            if \"commentsDisabled\" in error_msg:\n",
        "                print(f\"Comments are disabled for video {video_id}.\")\n",
        "                return []\n",
        "            else:\n",
        "                print(f\"Error fetching comments for {video_id}: {e}\")\n",
        "                time.sleep(10)\n",
        "                break\n",
        "\n",
        "        for item in response.get('items', []):\n",
        "            snippet = item['snippet']['topLevelComment']['snippet']\n",
        "            comment_data = {\n",
        "                'text': snippet.get('textDisplay', ''),\n",
        "                'author': snippet.get('authorDisplayName', 'Unknown'),\n",
        "                'timestamp': snippet.get('publishedAt', None)\n",
        "            }\n",
        "            comments.append(comment_data)\n",
        "\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Retrieve basic video details (author and publication date)\n",
        "def get_video_details(video_id, api_key):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key, cache_discovery=False)\n",
        "    try:\n",
        "        request = youtube.videos().list(\n",
        "            part='snippet',\n",
        "            id=video_id\n",
        "        )\n",
        "        response = request.execute()\n",
        "        time.sleep(1)\n",
        "    except HttpError as e:\n",
        "        print(f\"Error fetching details for {video_id}: {e}\")\n",
        "        time.sleep(10)\n",
        "        return None\n",
        "\n",
        "    items = response.get('items', [])\n",
        "    if not items:\n",
        "        return None\n",
        "    snippet = items[0]['snippet']\n",
        "    video_details = {\n",
        "        'author': snippet.get('channelTitle', 'Unknown'),\n",
        "        'createdAt': snippet.get('publishedAt', None)\n",
        "    }\n",
        "    return video_details\n",
        "\n",
        "# Process a video: extract transcript, comments, and metadata; then return rows\n",
        "def process_video(video_url, api_key):\n",
        "    print(f\"\\nProcessing video: {video_url}\")\n",
        "    video_id = extract_video_id(video_url)\n",
        "    print(f\"Video ID: {video_id}\")\n",
        "\n",
        "    transcript_text = get_transcript(video_id)\n",
        "    if transcript_text:\n",
        "        print(\"Transcript retrieved.\")\n",
        "    else:\n",
        "        print(\"No transcript available.\")\n",
        "\n",
        "    comments = get_video_comments(video_id, api_key)\n",
        "    print(f\"Number of comments retrieved: {len(comments)}\")\n",
        "\n",
        "    video_details = get_video_details(video_id, api_key)\n",
        "    if video_details:\n",
        "        video_author = video_details.get('author', 'Unknown')\n",
        "        video_published_at = video_details.get('createdAt', None)\n",
        "    else:\n",
        "        video_author = 'Unknown'\n",
        "        video_published_at = None\n",
        "\n",
        "    rows = []\n",
        "    # Add transcript as a \"news\" type row\n",
        "    if transcript_text:\n",
        "        rows.append({\n",
        "            'type': 'news',\n",
        "            'content': transcript_text,\n",
        "            'timestamp': video_published_at,\n",
        "            'author': video_author,\n",
        "            'post_url': video_url,\n",
        "            'platform': 'YouTube'\n",
        "        })\n",
        "\n",
        "    # Add each comment as a \"comment\" type row using its own author and timestamp\n",
        "    for comment in comments:\n",
        "        rows.append({\n",
        "            'type': 'comment',\n",
        "            'content': comment['text'],\n",
        "            'timestamp': comment['timestamp'],\n",
        "            'author': comment['author'],\n",
        "            'post_url': video_url,\n",
        "            'platform': 'YouTube'\n",
        "        })\n",
        "\n",
        "    return rows\n",
        "\n",
        "# Analyze YouTube video URLs and merge all data into one final CSV\n",
        "def analyze_yt_videos():\n",
        "    youtube_urls_df = pd.read_csv(\"youtubeVideo_links.csv\")\n",
        "    youtube_urls = youtube_urls_df['url'].dropna().tolist()\n",
        "\n",
        "    api_key = \"AIzaSyCzduWRq-77kXXbsUt8WCUOZPONfo1UPC0\"\n",
        "\n",
        "    summary_rows = []\n",
        "\n",
        "    for url in youtube_urls:\n",
        "        video_rows = process_video(url, api_key)\n",
        "        summary_rows.extend(video_rows)\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Create final DataFrame with specified column order\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    final_columns = ['type', 'content', 'timestamp', 'author', 'post_url', 'platform']\n",
        "    summary_df = summary_df[final_columns]\n",
        "    final_csv = \"videos_details_summary.csv\"\n",
        "    summary_df.to_csv(final_csv, index=False)\n",
        "    print(f\"Overall summary saved to {final_csv}\")\n",
        "    return final_csv\n",
        "\n",
        "# Search for a channel using the YouTube Data API\n",
        "def search_channel():\n",
        "    url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "    params = {\n",
        "        \"part\": \"snippet\",\n",
        "        \"q\": \"@preyashyadav\",\n",
        "        \"type\": \"channel\",\n",
        "        \"key\": \"AIzaSyCzduWRq-77kXXbsUt8WCUOZPONfo1UPC0\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        time.sleep(1)\n",
        "        data = response.json()\n",
        "        print(data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during channel search: {e}\")\n",
        "\n",
        "# Obtain details about a specific channel using the YouTube Data API\n",
        "def get_channel_details():\n",
        "    channel_id = \"UCkDii4wjOVlQAsK1a3kxN1A\"\n",
        "    url = \"https://www.googleapis.com/youtube/v3/channels\"\n",
        "    params = {\n",
        "        \"part\": \"snippet\",\n",
        "        \"id\": channel_id,\n",
        "        \"key\": \"AIzaSyCzduWRq-77kXXbsUt8WCUOZPONfo1UPC0\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        time.sleep(1)\n",
        "        channel_data = response.json()\n",
        "        print(channel_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching channel details: {e}\")\n",
        "\n",
        "# Run video analysis and perform queries\n",
        "final_csv = analyze_yt_videos()\n",
        "search_channel()\n",
        "get_channel_details()\n",
        "\n",
        "# Download the final CSV file (for example, in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(final_csv)\n",
        "except ImportError:\n",
        "    print(\"File download is not available in this environment. Please manually download the CSV file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JL-O6J75Rglv",
        "outputId": "ff1e6af4-8a59-410d-b573-8ecb6abaed75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Q4HnFkmRDCw\n",
            "Video ID: Q4HnFkmRDCw\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 32\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=mroFnhe6olo\n",
            "Video ID: mroFnhe6olo\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=bQyx0F2bT9A\n",
            "Video ID: bQyx0F2bT9A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video bQyx0F2bT9A.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=JqxtqfjvBMA\n",
            "Video ID: JqxtqfjvBMA\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 315\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=y1jrt5jKApI\n",
            "Video ID: y1jrt5jKApI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video y1jrt5jKApI.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=3Brp8xn1ko4\n",
            "Video ID: 3Brp8xn1ko4\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 616\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=ukkP8GtN6pk\n",
            "Video ID: ukkP8GtN6pk\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 174\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=SBVNzZjoI50\n",
            "Video ID: SBVNzZjoI50\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 9\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Q4Gh2n2HBmc\n",
            "Video ID: Q4Gh2n2HBmc\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=xjv5uNwmHbI\n",
            "Video ID: xjv5uNwmHbI\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 12\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=GgdKgb5Le1Y\n",
            "Video ID: GgdKgb5Le1Y\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 108\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=E9K7v4V2ZSc\n",
            "Video ID: E9K7v4V2ZSc\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 11\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=-n0qGeBwpTo\n",
            "Video ID: -n0qGeBwpTo\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 40\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=nDTvv65b8Ys\n",
            "Video ID: nDTvv65b8Ys\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=TJ2T_ITlexo\n",
            "Video ID: TJ2T_ITlexo\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=ekGzJgNQrTM\n",
            "Video ID: ekGzJgNQrTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video ekGzJgNQrTM.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=45FccKLoNzc\n",
            "Video ID: 45FccKLoNzc\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 24\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=cxNE0hqAfcs\n",
            "Video ID: cxNE0hqAfcs\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 11\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=OF-QagxsCcU\n",
            "Video ID: OF-QagxsCcU\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 8\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=3dRVBKm2BH0\n",
            "Video ID: 3dRVBKm2BH0\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 58\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=5rnzskmZVB8\n",
            "Video ID: 5rnzskmZVB8\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=yTz2sIv_v68\n",
            "Video ID: yTz2sIv_v68\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 8\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=T9UvsMEQotM\n",
            "Video ID: T9UvsMEQotM\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 264\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=1iG8ir-ztdo\n",
            "Video ID: 1iG8ir-ztdo\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 6\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=bvYQTf28zXQ\n",
            "Video ID: bvYQTf28zXQ\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 4\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=idwigrU3BQ8\n",
            "Video ID: idwigrU3BQ8\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=9mWzibZwocg\n",
            "Video ID: 9mWzibZwocg\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 10\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=uouhMdlE2rg\n",
            "Video ID: uouhMdlE2rg\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 159\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=MAXyygpUor0\n",
            "Video ID: MAXyygpUor0\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 29\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=_ztSlF_J6rA\n",
            "Video ID: _ztSlF_J6rA\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 1\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=sOzepEHLR80\n",
            "Video ID: sOzepEHLR80\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 1\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=dN9ZGNfkZSg\n",
            "Video ID: dN9ZGNfkZSg\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 11\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=0xLewVeDpUQ\n",
            "Video ID: 0xLewVeDpUQ\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 112\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=pQBKoWfclEM\n",
            "Video ID: pQBKoWfclEM\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 39\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=UDM_x6Snq-c\n",
            "Video ID: UDM_x6Snq-c\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 2628\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=wvWKb52cZ28\n",
            "Video ID: wvWKb52cZ28\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 2\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=LIP0-oLhAQQ\n",
            "Video ID: LIP0-oLhAQQ\n",
            "English transcript not available for video LIP0-oLhAQQ: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=LIP0-oLhAQQ! This is most likely caused by:\n",
            "\n",
            "No transcripts were found for any of the requested language codes: ['en']\n",
            "\n",
            "For this video (LIP0-oLhAQQ) transcripts are available in the following languages:\n",
            "\n",
            "(MANUALLY CREATED)\n",
            " - es-419 (\"Spanish (Latin America)\")[TRANSLATABLE]\n",
            "\n",
            "(GENERATED)\n",
            " - es (\"Spanish (auto-generated)\")[TRANSLATABLE]\n",
            "\n",
            "(TRANSLATION LANGUAGES)\n",
            " - ab (\"Abkhazian\")\n",
            " - aa (\"Afar\")\n",
            " - af (\"Afrikaans\")\n",
            " - ak (\"Akan\")\n",
            " - sq (\"Albanian\")\n",
            " - am (\"Amharic\")\n",
            " - ar (\"Arabic\")\n",
            " - hy (\"Armenian\")\n",
            " - as (\"Assamese\")\n",
            " - ay (\"Aymara\")\n",
            " - az (\"Azerbaijani\")\n",
            " - bn (\"Bangla\")\n",
            " - ba (\"Bashkir\")\n",
            " - eu (\"Basque\")\n",
            " - be (\"Belarusian\")\n",
            " - bho (\"Bhojpuri\")\n",
            " - bs (\"Bosnian\")\n",
            " - br (\"Breton\")\n",
            " - bg (\"Bulgarian\")\n",
            " - my (\"Burmese\")\n",
            " - ca (\"Catalan\")\n",
            " - ceb (\"Cebuano\")\n",
            " - zh-Hans (\"Chinese (Simplified)\")\n",
            " - zh-Hant (\"Chinese (Traditional)\")\n",
            " - co (\"Corsican\")\n",
            " - hr (\"Croatian\")\n",
            " - cs (\"Czech\")\n",
            " - da (\"Danish\")\n",
            " - dv (\"Divehi\")\n",
            " - nl (\"Dutch\")\n",
            " - dz (\"Dzongkha\")\n",
            " - en (\"English\")\n",
            " - eo (\"Esperanto\")\n",
            " - et (\"Estonian\")\n",
            " - ee (\"Ewe\")\n",
            " - fo (\"Faroese\")\n",
            " - fj (\"Fijian\")\n",
            " - fil (\"Filipino\")\n",
            " - fi (\"Finnish\")\n",
            " - fr (\"French\")\n",
            " - gaa (\"Ga\")\n",
            " - gl (\"Galician\")\n",
            " - lg (\"Ganda\")\n",
            " - ka (\"Georgian\")\n",
            " - de (\"German\")\n",
            " - el (\"Greek\")\n",
            " - gn (\"Guarani\")\n",
            " - gu (\"Gujarati\")\n",
            " - ht (\"Haitian Creole\")\n",
            " - ha (\"Hausa\")\n",
            " - haw (\"Hawaiian\")\n",
            " - iw (\"Hebrew\")\n",
            " - hi (\"Hindi\")\n",
            " - hmn (\"Hmong\")\n",
            " - hu (\"Hungarian\")\n",
            " - is (\"Icelandic\")\n",
            " - ig (\"Igbo\")\n",
            " - id (\"Indonesian\")\n",
            " - iu (\"Inuktitut\")\n",
            " - ga (\"Irish\")\n",
            " - it (\"Italian\")\n",
            " - ja (\"Japanese\")\n",
            " - jv (\"Javanese\")\n",
            " - kl (\"Kalaallisut\")\n",
            " - kn (\"Kannada\")\n",
            " - kk (\"Kazakh\")\n",
            " - kha (\"Khasi\")\n",
            " - km (\"Khmer\")\n",
            " - rw (\"Kinyarwanda\")\n",
            " - ko (\"Korean\")\n",
            " - kri (\"Krio\")\n",
            " - ku (\"Kurdish\")\n",
            " - ky (\"Kyrgyz\")\n",
            " - lo (\"Lao\")\n",
            " - la (\"Latin\")\n",
            " - lv (\"Latvian\")\n",
            " - ln (\"Lingala\")\n",
            " - lt (\"Lithuanian\")\n",
            " - lua (\"Luba-Lulua\")\n",
            " - luo (\"Luo\")\n",
            " - lb (\"Luxembourgish\")\n",
            " - mk (\"Macedonian\")\n",
            " - mg (\"Malagasy\")\n",
            " - ms (\"Malay\")\n",
            " - ml (\"Malayalam\")\n",
            " - mt (\"Maltese\")\n",
            " - gv (\"Manx\")\n",
            " - mi (\"Māori\")\n",
            " - mr (\"Marathi\")\n",
            " - mn (\"Mongolian\")\n",
            " - mfe (\"Morisyen\")\n",
            " - ne (\"Nepali\")\n",
            " - new (\"Newari\")\n",
            " - nso (\"Northern Sotho\")\n",
            " - no (\"Norwegian\")\n",
            " - ny (\"Nyanja\")\n",
            " - oc (\"Occitan\")\n",
            " - or (\"Odia\")\n",
            " - om (\"Oromo\")\n",
            " - os (\"Ossetic\")\n",
            " - pam (\"Pampanga\")\n",
            " - ps (\"Pashto\")\n",
            " - fa (\"Persian\")\n",
            " - pl (\"Polish\")\n",
            " - pt (\"Portuguese\")\n",
            " - pt-PT (\"Portuguese (Portugal)\")\n",
            " - pa (\"Punjabi\")\n",
            " - qu (\"Quechua\")\n",
            " - ro (\"Romanian\")\n",
            " - rn (\"Rundi\")\n",
            " - ru (\"Russian\")\n",
            " - sm (\"Samoan\")\n",
            " - sg (\"Sango\")\n",
            " - sa (\"Sanskrit\")\n",
            " - gd (\"Scottish Gaelic\")\n",
            " - sr (\"Serbian\")\n",
            " - crs (\"Seselwa Creole French\")\n",
            " - sn (\"Shona\")\n",
            " - sd (\"Sindhi\")\n",
            " - si (\"Sinhala\")\n",
            " - sk (\"Slovak\")\n",
            " - sl (\"Slovenian\")\n",
            " - so (\"Somali\")\n",
            " - st (\"Southern Sotho\")\n",
            " - es (\"Spanish\")\n",
            " - su (\"Sundanese\")\n",
            " - sw (\"Swahili\")\n",
            " - ss (\"Swati\")\n",
            " - sv (\"Swedish\")\n",
            " - tg (\"Tajik\")\n",
            " - ta (\"Tamil\")\n",
            " - tt (\"Tatar\")\n",
            " - te (\"Telugu\")\n",
            " - th (\"Thai\")\n",
            " - bo (\"Tibetan\")\n",
            " - ti (\"Tigrinya\")\n",
            " - to (\"Tongan\")\n",
            " - ts (\"Tsonga\")\n",
            " - tn (\"Tswana\")\n",
            " - tum (\"Tumbuka\")\n",
            " - tr (\"Turkish\")\n",
            " - tk (\"Turkmen\")\n",
            " - uk (\"Ukrainian\")\n",
            " - ur (\"Urdu\")\n",
            " - ug (\"Uyghur\")\n",
            " - uz (\"Uzbek\")\n",
            " - ve (\"Venda\")\n",
            " - vi (\"Vietnamese\")\n",
            " - war (\"Waray\")\n",
            " - cy (\"Welsh\")\n",
            " - fy (\"Western Frisian\")\n",
            " - wo (\"Wolof\")\n",
            " - xh (\"Xhosa\")\n",
            " - yi (\"Yiddish\")\n",
            " - yo (\"Yoruba\")\n",
            " - zu (\"Zulu\")\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Spanish transcript retrieved. Translating to English...\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 7\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=kh1p4GDDXbI\n",
            "Video ID: kh1p4GDDXbI\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 1801\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=jJf18gsxFFk\n",
            "Video ID: jJf18gsxFFk\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 350\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=oEWYw3JUfME\n",
            "Video ID: oEWYw3JUfME\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video oEWYw3JUfME.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=SpgQ-GWItiE\n",
            "Video ID: SpgQ-GWItiE\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 43\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=WWNJtjqgURU\n",
            "Video ID: WWNJtjqgURU\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 119\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=9UWAlOvFEbI\n",
            "Video ID: 9UWAlOvFEbI\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Yv9gzhoT59E\n",
            "Video ID: Yv9gzhoT59E\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 8\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=2QKwZyK56nA\n",
            "Video ID: 2QKwZyK56nA\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 7\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=0immYy4mVzQ\n",
            "Video ID: 0immYy4mVzQ\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=SRzKCm0hmjU\n",
            "Video ID: SRzKCm0hmjU\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 129\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=vSyRtyk6Cxo\n",
            "Video ID: vSyRtyk6Cxo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video vSyRtyk6Cxo.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=iMlPHsa5XdU\n",
            "Video ID: iMlPHsa5XdU\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 6\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=dkdUfSzdCDM\n",
            "Video ID: dkdUfSzdCDM\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 436\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=nGRSgXObUc8\n",
            "Video ID: nGRSgXObUc8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video nGRSgXObUc8.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=8vI-5zzNkP0\n",
            "Video ID: 8vI-5zzNkP0\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=w21TLQOIThk\n",
            "Video ID: w21TLQOIThk\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 27\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=QA-laFwTCoc\n",
            "Video ID: QA-laFwTCoc\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 906\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=z40rbOLUi-w\n",
            "Video ID: z40rbOLUi-w\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 91\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=rLgWLXzzM0Q\n",
            "Video ID: rLgWLXzzM0Q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript retrieved.\n",
            "Comments are disabled for video rLgWLXzzM0Q.\n",
            "Number of comments retrieved: 0\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=44UsjFNQt4Q\n",
            "Video ID: 44UsjFNQt4Q\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 17\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=W_aUPoFkULs\n",
            "Video ID: W_aUPoFkULs\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 5\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=gut_Gys7Pbo\n",
            "Video ID: gut_Gys7Pbo\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 414\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Kx2DG4YtbF4\n",
            "Video ID: Kx2DG4YtbF4\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 318\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=iHBf8Vb8bmY\n",
            "Video ID: iHBf8Vb8bmY\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 12\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Pus_mbDP4Zo\n",
            "Video ID: Pus_mbDP4Zo\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 5252\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=cKHi4cFXAc0\n",
            "Video ID: cKHi4cFXAc0\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 67\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=CWze2xQaqcw\n",
            "Video ID: CWze2xQaqcw\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 204\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Y9xwZRV12Nw\n",
            "Video ID: Y9xwZRV12Nw\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 59\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=Ccpr1TOzzI0\n",
            "Video ID: Ccpr1TOzzI0\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 15\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=UfyPtvZGHAI\n",
            "Video ID: UfyPtvZGHAI\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 3274\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=BI84E354Yc8\n",
            "Video ID: BI84E354Yc8\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 8\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=AwoBakHU2vQ\n",
            "Video ID: AwoBakHU2vQ\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 6\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=QYU0n9F7pHk\n",
            "Video ID: QYU0n9F7pHk\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 3650\n",
            "\n",
            "Processing video: https://www.youtube.com/watch?v=_fgZ-idXUeY\n",
            "Video ID: _fgZ-idXUeY\n",
            "Transcript retrieved.\n",
            "Number of comments retrieved: 2263\n",
            "Overall summary saved to videos_details_summary.csv\n",
            "{'kind': 'youtube#searchListResponse', 'etag': '7dKo_rDeREXeRxBLVxY6ene05qI', 'regionCode': 'US', 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}, 'items': [{'kind': 'youtube#searchResult', 'etag': 'TdA5fOyEIeyamBDYdnZu4seH9QU', 'id': {'kind': 'youtube#channel', 'channelId': 'UCkDii4wjOVlQAsK1a3kxN1A'}, 'snippet': {'publishedAt': '2019-03-13T07:12:08Z', 'channelId': 'UCkDii4wjOVlQAsK1a3kxN1A', 'title': 'Preyash Yadav', 'description': \"Welcome to my channel! About Me: Hey there! I'm Preyash Yadav, a passionate computer science enthusiast and AI research ...\", 'thumbnails': {'default': {'url': 'https://yt3.ggpht.com/ytc/AIdro_keBWUqE90cNDkqaxlgvDDAhwW6RNzG9Az9MO-rjd4RVDs=s88-c-k-c0xffffffff-no-rj-mo'}, 'medium': {'url': 'https://yt3.ggpht.com/ytc/AIdro_keBWUqE90cNDkqaxlgvDDAhwW6RNzG9Az9MO-rjd4RVDs=s240-c-k-c0xffffffff-no-rj-mo'}, 'high': {'url': 'https://yt3.ggpht.com/ytc/AIdro_keBWUqE90cNDkqaxlgvDDAhwW6RNzG9Az9MO-rjd4RVDs=s800-c-k-c0xffffffff-no-rj-mo'}}, 'channelTitle': 'Preyash Yadav', 'liveBroadcastContent': 'none', 'publishTime': '2019-03-13T07:12:08Z'}}]}\n",
            "{'kind': 'youtube#channelListResponse', 'etag': 'fXDqEz_dQb1TTBl-EqQ437pmthk', 'pageInfo': {'totalResults': 1, 'resultsPerPage': 5}, 'items': [{'kind': 'youtube#channel', 'etag': 'E9llaHfFKViihOmcljgJblUBscw', 'id': 'UCkDii4wjOVlQAsK1a3kxN1A', 'snippet': {'title': 'Preyash Yadav', 'description': \"Welcome to my channel!\\n🚀 About Me:\\nHey there! I'm Preyash Yadav, a passionate computer science enthusiast and AI research aficionado. Currently, I'm an undergraduate student at VIT Chennai, diving deep into the world of technology and innovation. As a Full Stack Developer intern at VIT SpoRIC and an Undergraduate Research Intern at VIT CCPS, I'm constantly exploring the realms of computer science, pushing boundaries, and embracing the endless possibilities that technology offers.\\n\\n🔍 What You Can Expect:\\nOn this channel, I bring you a treasure trove of computer science-related content, from coding tutorials and programming challenges to in-depth discussions about AI research and its fascinating applications. Whether you're a beginner eager to learn the basics of programming or a tech enthusiast hungry for the latest advancements in artificial intelligence, you're in the right place.\\nHappy coding!!\\n\", 'customUrl': '@preyashyadav', 'publishedAt': '2019-03-13T07:12:08Z', 'thumbnails': {'default': {'url': 'https://yt3.ggpht.com/ytc/AIdro_keBWUqE90cNDkqaxlgvDDAhwW6RNzG9Az9MO-rjd4RVDs=s88-c-k-c0x00ffffff-no-rj', 'width': 88, 'height': 88}, 'medium': {'url': 'https://yt3.ggpht.com/ytc/AIdro_keBWUqE90cNDkqaxlgvDDAhwW6RNzG9Az9MO-rjd4RVDs=s240-c-k-c0x00ffffff-no-rj', 'width': 240, 'height': 240}, 'high': {'url': 'https://yt3.ggpht.com/ytc/AIdro_keBWUqE90cNDkqaxlgvDDAhwW6RNzG9Az9MO-rjd4RVDs=s800-c-k-c0x00ffffff-no-rj', 'width': 800, 'height': 800}}, 'localized': {'title': 'Preyash Yadav', 'description': \"Welcome to my channel!\\n🚀 About Me:\\nHey there! I'm Preyash Yadav, a passionate computer science enthusiast and AI research aficionado. Currently, I'm an undergraduate student at VIT Chennai, diving deep into the world of technology and innovation. As a Full Stack Developer intern at VIT SpoRIC and an Undergraduate Research Intern at VIT CCPS, I'm constantly exploring the realms of computer science, pushing boundaries, and embracing the endless possibilities that technology offers.\\n\\n🔍 What You Can Expect:\\nOn this channel, I bring you a treasure trove of computer science-related content, from coding tutorials and programming challenges to in-depth discussions about AI research and its fascinating applications. Whether you're a beginner eager to learn the basics of programming or a tech enthusiast hungry for the latest advancements in artificial intelligence, you're in the right place.\\nHappy coding!!\\n\"}}}]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_52b27da7-f2dc-462e-ace1-66cb0a142d96\", \"videos_details_summary.csv\", 6114247)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis on Scraped Transcript and comments of Youtube**"
      ],
      "metadata": {
        "id": "alQipJY9Qkh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon (if not already downloaded)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Define the input CSV file and output CSV file\n",
        "input_file = \"/content/videos_details_summary.csv\"\n",
        "output_file = \"/content/videos_details_summary_with_sentiment.csv\"\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Textual data preprocessing: strip extra whitespace\n",
        "def data_preprocessing(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.strip()\n",
        "    return text\n",
        "\n",
        "# Read the input CSV file\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Calculate the compound sentiment score for each row's content\n",
        "df['sentiment'] = df['content'].apply(\n",
        "    lambda text: sid.polarity_scores(data_preprocessing(text))['compound'] if isinstance(text, str) else None\n",
        ")\n",
        "\n",
        "# Rearrange columns to insert 'sentiment' between 'post_url' and 'platform'\n",
        "# Assuming original columns order: type, content, timestamp, author, post_url, platform\n",
        "# New order: type, content, timestamp, author, post_url, sentiment, platform\n",
        "desired_columns = ['type', 'content', 'timestamp', 'author', 'post_url', 'sentiment', 'platform']\n",
        "df = df[desired_columns]\n",
        "\n",
        "# Save the resulting DataFrame to a new CSV file\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Sentiment analysis complete. Output saved to {output_file}\")\n",
        "\n",
        "# Attempt to download the file (e.g., for Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_file)\n",
        "except ImportError:\n",
        "    print(\"File download is not available in this environment. Please manually download the CSV file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DT1_f88zWzJy",
        "outputId": "14dd7bba-cc40-4f19-e0cf-295349f2df79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete. Output saved to /content/videos_details_summary_with_sentiment.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d0386c04-afec-4dba-aca4-039a444a3175\", \"videos_details_summary_with_sentiment.csv\", 6270652)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}